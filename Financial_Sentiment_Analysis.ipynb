{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "mLgTWLbwAnn-",
        "outputId": "5c7e0f54-808a-42b3-c4da-9bb0e1b74656"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/labeled.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-271593541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load labeled dataset (with sentiment scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabeled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/labeled.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the first few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/labeled.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load labeled dataset (with sentiment scores)\n",
        "labeled_df = pd.read_csv('/content/labeled.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Labeled Dataset:\")\n",
        "display(labeled_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOWsrAWBAqik"
      },
      "outputs": [],
      "source": [
        "# Check basic info\n",
        "print(\"Labeled Dataset Info:\")\n",
        "print(labeled_df.info())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values in Labeled Dataset:\")\n",
        "print(labeled_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVn8uojWAu3J"
      },
      "outputs": [],
      "source": [
        "# Drop the unnecessary column\n",
        "labeled_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "# Fill missing 'body' values correctly to avoid warning\n",
        "labeled_df.loc[:, 'body'] = labeled_df['body'].fillna(labeled_df['title'] + \" \" + labeled_df['intro'])\n",
        "\n",
        "# Confirm there are no missing values now\n",
        "print(\"\\nMissing Values After Cleaning (Labeled Dataset):\")\n",
        "print(labeled_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVMHrNGCAw26"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot sentiment score distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(labeled_df['score'], bins=30, kde=True)\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBS_JL9oAziH"
      },
      "outputs": [],
      "source": [
        "# Define sentiment categories\n",
        "def classify_sentiment(score):\n",
        "    if score > 0.2:\n",
        "        return 'Positive'\n",
        "    elif score < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply function to create a new sentiment label column\n",
        "labeled_df['sentiment_label'] = labeled_df['score'].apply(classify_sentiment)\n",
        "\n",
        "# Check the distribution of sentiment labels\n",
        "print(labeled_df['sentiment_label'].value_counts())\n",
        "\n",
        "# Visualizing sentiment label distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=labeled_df['sentiment_label'], palette='viridis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Sentiment Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBPk8iJcBOw8"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets gcsfs fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKiguhiauDP5"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio fastai -y\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmAi1FlJBPTV",
        "outputId": "da479726-fd86-4114-ae4c-e054f890564f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.1\n",
            "2.8.0+cu126\n",
            "0.23.0+cu126\n",
            "2.8.0+cu126\n",
            "2.8.4\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import fastai\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(fastai.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uiLPTnTBPV8"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load FinBERT tokenizer & model\n",
        "MODEL_NAME = \"ProsusAI/finbert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)  # 3 labels: Positive, Neutral, Negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OfByfR9BPYk"
      },
      "outputs": [],
      "source": [
        "# Convert sentiment labels to numerical format\n",
        "label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "labeled_df['sentiment_label'] = labeled_df['sentiment_label'].map(label_mapping)\n",
        "\n",
        "# Ensure the labels are integers\n",
        "labeled_df['sentiment_label'] = labeled_df['sentiment_label'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DvaWKGFBPbF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04fIXhzxBPet"
      },
      "outputs": [],
      "source": [
        "# Reset index before splitting to avoid index mismatch issues\n",
        "labeled_df = labeled_df.reset_index(drop=True)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    labeled_df['body'],  # Use raw text data\n",
        "    labeled_df['sentiment_label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpmCkD0HBeT5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the dataset class\n",
        "class FinancialNewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = FinancialNewsDataset(\n",
        "    texts=X_train.tolist(),\n",
        "    labels=y_train.tolist(),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = FinancialNewsDataset(\n",
        "    texts=X_test.tolist(),\n",
        "    labels=y_test.tolist(),\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxvdkqz5Be0f"
      },
      "outputs": [],
      "source": [
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGtdwTPtBe3N"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "scaler = GradScaler()  # Helps prevent FP16 instability\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, leave=True)  # Show training progress\n",
        "\n",
        "    for batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # Enables mixed precision\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch {epoch+1}\")\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q71ia4lKBe6E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Extract FinBERT logits from training set\n",
        "xgb_train_features = []\n",
        "xgb_train_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        xgb_train_features.extend(logits.cpu().numpy())\n",
        "        xgb_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Step 2: Train XGBoost on FinBERT logits\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_model.fit(xgb_train_features, xgb_train_labels)\n",
        "\n",
        "# Step 3: Evaluate both FinBERT and XGBoost on test set\n",
        "correct, total = 0, 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_predictions_proba = []\n",
        "xgb_test_features = []\n",
        "xgb_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_predictions_proba.extend(probs.cpu().numpy())\n",
        "\n",
        "        xgb_test_features.extend(logits.cpu().numpy())\n",
        "        xgb_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Step 4: FinBERT Metrics\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinBERT Test Accuracy: {accuracy:.4f}\")\n",
        "report = classification_report(all_labels, all_predictions, target_names=['Negative', 'Neutral', 'Positive'], digits=4)\n",
        "print(\"\\nFinBERT Classification Report:\\n\", report)\n",
        "\n",
        "# Step 5: XGBoost Metrics\n",
        "xgb_preds = xgb_model.predict(xgb_test_features)\n",
        "xgb_accuracy = accuracy_score(xgb_test_labels, xgb_preds)\n",
        "xgb_report = classification_report(xgb_test_labels, xgb_preds, target_names=['Negative', 'Neutral', 'Positive'], digits=4)\n",
        "\n",
        "print(f\"\\nXGBoost on FinBERT Logits - Test Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(\"\\nXGBoost Classification Report:\\n\", xgb_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUQBipbrtDm9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Generate dictionaries for FinBERT and XGBoost\n",
        "\n",
        "finbert_report_dict = classification_report(\n",
        "    all_labels,\n",
        "    all_predictions,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "xgb_report_dict = classification_report(\n",
        "    xgb_test_labels,\n",
        "    xgb_preds,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Step 2: Extract Precision, Recall, F1-Score\n",
        "\n",
        "finbert_precision = [\n",
        "    finbert_report_dict['Negative']['precision'],\n",
        "    finbert_report_dict['Neutral']['precision'],\n",
        "    finbert_report_dict['Positive']['precision'],\n",
        "]\n",
        "finbert_recall = [\n",
        "    finbert_report_dict['Negative']['recall'],\n",
        "    finbert_report_dict['Neutral']['recall'],\n",
        "    finbert_report_dict['Positive']['recall'],\n",
        "]\n",
        "finbert_f1 = [\n",
        "    finbert_report_dict['Negative']['f1-score'],\n",
        "    finbert_report_dict['Neutral']['f1-score'],\n",
        "    finbert_report_dict['Positive']['f1-score'],\n",
        "]\n",
        "\n",
        "xgb_precision = [\n",
        "    xgb_report_dict['Negative']['precision'],\n",
        "    xgb_report_dict['Neutral']['precision'],\n",
        "    xgb_report_dict['Positive']['precision'],\n",
        "]\n",
        "xgb_recall = [\n",
        "    xgb_report_dict['Negative']['recall'],\n",
        "    xgb_report_dict['Neutral']['recall'],\n",
        "    xgb_report_dict['Positive']['recall'],\n",
        "]\n",
        "xgb_f1 = [\n",
        "    xgb_report_dict['Negative']['f1-score'],\n",
        "    xgb_report_dict['Neutral']['f1-score'],\n",
        "    xgb_report_dict['Positive']['f1-score'],\n",
        "]\n",
        "\n",
        "# Step 3: Plotting\n",
        "\n",
        "# Set an overall style\n",
        "plt.style.use('seaborn-v0_8-deep')\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "colors_finbert = ['#1f77b4', '#1f77b4', '#1f77b4']  # blue shades\n",
        "colors_xgb = ['#ff7f0e', '#ff7f0e', '#ff7f0e']       # orange shades\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Precision\n",
        "axs[0].bar(x - width/2, finbert_precision, width, label='FinBERT', color=colors_finbert)\n",
        "axs[0].bar(x + width/2, xgb_precision, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[0].set_title('Precision Comparison', fontsize=14, fontweight='bold')\n",
        "axs[0].set_xticks(x)\n",
        "axs[0].set_xticklabels(labels, fontsize=12)\n",
        "axs[0].set_ylim(0, 1)\n",
        "axs[0].legend(fontsize=11)\n",
        "for bar in axs[0].containers:\n",
        "    axs[0].bar_label(bar, fmt='%.2f', padding=3, fontsize=10)\n",
        "\n",
        "# Recall\n",
        "axs[1].bar(x - width/2, finbert_recall, width, label='FinBERT', color=colors_finbert)\n",
        "axs[1].bar(x + width/2, xgb_recall, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[1].set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
        "axs[1].set_xticks(x)\n",
        "axs[1].set_xticklabels(labels, fontsize=12)\n",
        "axs[1].set_ylim(0, 1)\n",
        "axs[1].legend(fontsize=11)\n",
        "for bar in axs[1].containers:\n",
        "    axs[1].bar_label(bar, fmt='%.2f', padding=3, fontsize=10)\n",
        "\n",
        "# F1-Score\n",
        "axs[2].bar(x - width/2, finbert_f1, width, label='FinBERT', color=colors_finbert)\n",
        "axs[2].bar(x + width/2, xgb_f1, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[2].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
        "axs[2].set_xticks(x)\n",
        "axs[2].set_xticklabels(labels, fontsize=12)\n",
        "axs[2].set_ylim(0, 1)\n",
        "axs[2].legend(fontsize=11)\n",
        "for bar in axs[2].containers:\n",
        "    axs[2].bar_label(bar, fmt='%.2f', padding=3, fontsize=10)\n",
        "\n",
        "# Overall Figure Title\n",
        "fig.suptitle('Performance Metrics Comparison: FinBERT vs XGBoost', fontsize=18, fontweight='bold')\n",
        "\n",
        "# layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GukJeOvwBe8e"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syCBTBbrNZZ_"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load spaCy model for synonym replacement\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load MarianMT model for back translation\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer_mt = MarianTokenizer.from_pretrained(model_name)\n",
        "model_mt = MarianMTModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function for synonym replacement\n",
        "def synonym_replacement(text):\n",
        "    doc = nlp(text)\n",
        "    new_words = []\n",
        "    for token in doc:\n",
        "        if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"} and random.random() < 0.3:  # 30% chance to replace\n",
        "            syns = [syn.text for syn in token.vocab if syn.is_alpha and syn.text != token.text]\n",
        "            new_words.append(random.choice(syns) if syns else token.text)\n",
        "        else:\n",
        "            new_words.append(token.text)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# Function for back translation (using batch processing)\n",
        "def back_translate_batch(texts, batch_size=8):\n",
        "    translated_texts = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Back Translating\"):\n",
        "        batch = texts[i : i + batch_size]\n",
        "\n",
        "        # Step 1: Translate to French\n",
        "        inputs = tokenizer_mt(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=400).to(model_mt.device)\n",
        "        translated = model_mt.generate(**inputs, max_length=400)\n",
        "        fr_texts = tokenizer_mt.batch_decode(translated, skip_special_tokens=True)\n",
        "\n",
        "        # Step 2: Translate back to English\n",
        "        inputs_back = tokenizer_mt(fr_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=400).to(model_mt.device)\n",
        "        back_translated = model_mt.generate(**inputs_back, max_length=400)\n",
        "        translated_texts.extend(tokenizer_mt.batch_decode(back_translated, skip_special_tokens=True))\n",
        "\n",
        "    return translated_texts\n",
        "\n",
        "# Apply augmentation only to Negative & Neutral samples\n",
        "negative_neutral_texts = [text for text, label in zip(labeled_df['body'], labeled_df['sentiment_label']) if label in [0, 1]]\n",
        "negative_neutral_labels = [label for label in labeled_df['sentiment_label'] if label in [0, 1]]\n",
        "\n",
        "# Choose between synonym replacement and back translation\n",
        "augmented_texts = []\n",
        "for text in negative_neutral_texts:\n",
        "    if random.random() < 0.5:\n",
        "        augmented_texts.append(synonym_replacement(text))\n",
        "    else:\n",
        "        augmented_texts.append(text)  # Placeholder, replaced later with back translation\n",
        "\n",
        "# Perform back translation in batches\n",
        "back_translated_texts = back_translate_batch([text for text in augmented_texts if text], batch_size=8)\n",
        "\n",
        "# Replace placeholders with back-translated text\n",
        "idx = 0\n",
        "for i in range(len(augmented_texts)):\n",
        "    if augmented_texts[i] == \"\":\n",
        "        augmented_texts[i] = back_translated_texts[idx]\n",
        "        idx += 1\n",
        "\n",
        "# Create DataFrame for augmented data\n",
        "augmented_df = pd.DataFrame({\"body\": augmented_texts, \"sentiment_label\": negative_neutral_labels})\n",
        "\n",
        "# Append augmented data to original dataset\n",
        "labeled_df = pd.concat([labeled_df, augmented_df]).reset_index(drop=True)\n",
        "\n",
        "print(\"Data augmentation complete! New dataset size:\", len(labeled_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egqiwJSBoLea"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text data into numerical vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(labeled_df['body'])\n",
        "\n",
        "# Convert labels to numpy array\n",
        "y = labeled_df['sentiment_label'].values\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "# Get indices of resampled data\n",
        "resampled_indices = smote.fit_resample(np.arange(len(labeled_df)).reshape(-1, 1), y)[0].flatten()\n",
        "\n",
        "# Create the balanced DataFrame\n",
        "balanced_df = labeled_df.iloc[resampled_indices].reset_index(drop=True)\n",
        "\n",
        "print(\"SMOTE applied successfully!\")\n",
        "print(\"New class distribution:\\n\", balanced_df['sentiment_label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTbV1cvKBzwl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# After augmentation completed\n",
        "print(f\"Final Dataset Size: {len(balanced_df)}\")\n",
        "\n",
        "# Split the dataset\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    balanced_df['body'].tolist(),\n",
        "    balanced_df['sentiment_label'].tolist(),\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create Datasets and Loaders\n",
        "train_dataset = FinancialNewsDataset(train_texts, train_labels, tokenizer, max_length=384)\n",
        "val_dataset = FinancialNewsDataset(val_texts, val_labels, tokenizer, max_length=384)\n",
        "test_dataset = FinancialNewsDataset(test_texts, test_labels, tokenizer, max_length=384)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Learning Rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2XGiiKsBz2F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Adjusted Class Weights\n",
        "adjusted_class_weights = torch.tensor([1.9, 1.6, 0.5], dtype=torch.float).to(device)\n",
        "\n",
        "# Define loss function with updated weights\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=adjusted_class_weights)\n",
        "\n",
        "print(\"New Adjusted Class Weights:\", adjusted_class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxx5kadCBz4q"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "scaler = GradScaler()\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loop.set_description(f\"Epoch {epoch+1}\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # VALIDATION\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nModel training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJMHXWQSvJ_Q"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Extract FinBERT logits from training set for XGBoost\n",
        "xgb_train_features = []\n",
        "xgb_train_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # shape: [batch_size, 3]\n",
        "\n",
        "        xgb_train_features.extend(logits.cpu().numpy())\n",
        "        xgb_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Step 2: Train XGBoost on FinBERT logits\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_model.fit(xgb_train_features, xgb_train_labels)\n",
        "\n",
        "# Step 3: Extract FinBERT logits from test set\n",
        "xgb_test_features = []\n",
        "xgb_test_labels = []\n",
        "\n",
        "correct, total = 0, 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_predictions_proba = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_predictions_proba.extend(probs.cpu().numpy())\n",
        "\n",
        "        xgb_test_features.extend(logits.cpu().numpy())\n",
        "        xgb_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Step 4: Evaluate FinBERT Accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinBERT Test Accuracy: {accuracy:.4f}\")\n",
        "report = classification_report(all_labels, all_predictions, target_names=['Negative', 'Neutral', 'Positive'], digits=4)\n",
        "print(\"\\nFinBERT Classification Report:\\n\", report)\n",
        "\n",
        "# Step 5: Evaluate XGBoost on FinBERT logits\n",
        "xgb_preds = xgb_model.predict(xgb_test_features)\n",
        "xgb_accuracy = accuracy_score(xgb_test_labels, xgb_preds)\n",
        "xgb_report = classification_report(xgb_test_labels, xgb_preds, target_names=['Negative', 'Neutral', 'Positive'], digits=4)\n",
        "\n",
        "print(f\"\\nXGBoost on FinBERT Logits - Test Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(\"\\nXGBoost Classification Report:\\n\", xgb_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFItSWlF4suS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Generate dictionaries for FinBERT and XGBoost\n",
        "\n",
        "finbert_report_dict = classification_report(\n",
        "    all_labels,\n",
        "    all_predictions,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "xgb_report_dict = classification_report(\n",
        "    xgb_test_labels,\n",
        "    xgb_preds,\n",
        "    target_names=['Negative', 'Neutral', 'Positive'],\n",
        "    digits=4,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Step 2: Extract Precision, Recall, F1-Score\n",
        "\n",
        "finbert_precision = [\n",
        "    finbert_report_dict['Negative']['precision'],\n",
        "    finbert_report_dict['Neutral']['precision'],\n",
        "    finbert_report_dict['Positive']['precision'],\n",
        "]\n",
        "finbert_recall = [\n",
        "    finbert_report_dict['Negative']['recall'],\n",
        "    finbert_report_dict['Neutral']['recall'],\n",
        "    finbert_report_dict['Positive']['recall'],\n",
        "]\n",
        "finbert_f1 = [\n",
        "    finbert_report_dict['Negative']['f1-score'],\n",
        "    finbert_report_dict['Neutral']['f1-score'],\n",
        "    finbert_report_dict['Positive']['f1-score'],\n",
        "]\n",
        "\n",
        "xgb_precision = [\n",
        "    xgb_report_dict['Negative']['precision'],\n",
        "    xgb_report_dict['Neutral']['precision'],\n",
        "    xgb_report_dict['Positive']['precision'],\n",
        "]\n",
        "xgb_recall = [\n",
        "    xgb_report_dict['Negative']['recall'],\n",
        "    xgb_report_dict['Neutral']['recall'],\n",
        "    xgb_report_dict['Positive']['recall'],\n",
        "]\n",
        "xgb_f1 = [\n",
        "    xgb_report_dict['Negative']['f1-score'],\n",
        "    xgb_report_dict['Neutral']['f1-score'],\n",
        "    xgb_report_dict['Positive']['f1-score'],\n",
        "]\n",
        "\n",
        "# Step 3: Plotting\n",
        "\n",
        "# Set an overall style\n",
        "plt.style.use('seaborn-v0_8-deep')\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "colors_finbert = ['#2ca02c', '#2ca02c', '#2ca02c']  # green shades (for FinBERT)\n",
        "colors_xgb = ['#d62728', '#d62728', '#d62728']       # red shades (for XGBoost)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Precision\n",
        "axs[0].bar(x - width/2, finbert_precision, width, label='FinBERT', color=colors_finbert)\n",
        "axs[0].bar(x + width/2, xgb_precision, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[0].set_title('Precision Comparison', fontsize=14, fontweight='bold')\n",
        "axs[0].set_xticks(x)\n",
        "axs[0].set_xticklabels(labels, fontsize=12)\n",
        "axs[0].set_ylim(0.85, 1.0)\n",
        "axs[0].legend(fontsize=11)\n",
        "for bar in axs[0].containers:\n",
        "    axs[0].bar_label(bar, fmt='%.4f', padding=3, fontsize=10)\n",
        "\n",
        "# Recall\n",
        "axs[1].bar(x - width/2, finbert_recall, width, label='FinBERT', color=colors_finbert)\n",
        "axs[1].bar(x + width/2, xgb_recall, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[1].set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
        "axs[1].set_xticks(x)\n",
        "axs[1].set_xticklabels(labels, fontsize=12)\n",
        "axs[1].set_ylim(0.85, 1.0)\n",
        "axs[1].legend(fontsize=11)\n",
        "for bar in axs[1].containers:\n",
        "    axs[1].bar_label(bar, fmt='%.4f', padding=3, fontsize=10)\n",
        "\n",
        "# F1-Score\n",
        "axs[2].bar(x - width/2, finbert_f1, width, label='FinBERT', color=colors_finbert)\n",
        "axs[2].bar(x + width/2, xgb_f1, width, label='XGBoost on FinBERT', color=colors_xgb)\n",
        "axs[2].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
        "axs[2].set_xticks(x)\n",
        "axs[2].set_xticklabels(labels, fontsize=12)\n",
        "axs[2].set_ylim(0.85, 1.0)\n",
        "axs[2].legend(fontsize=11)\n",
        "for bar in axs[2].containers:\n",
        "    axs[2].bar_label(bar, fmt='%.4f', padding=3, fontsize=10)\n",
        "\n",
        "# Overall Figure Title\n",
        "fig.suptitle('FinBERT vs XGBoost on FinBERT (Test Set)', fontsize=18, fontweight='bold')\n",
        "\n",
        "# layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htW64IppkUUR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, classification_report\n",
        "\n",
        "# Convert Model Outputs to Probabilities (For PR Curve)\n",
        "model.eval()\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)  # Convert logits to probabilities\n",
        "\n",
        "        all_probabilities.extend(probs.cpu().numpy())\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "y_true = np.array(all_labels)  # Already collected in evaluation\n",
        "y_pred = np.array(all_predictions)\n",
        "y_pred_proba = np.array(all_probabilities)  # Model probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wET5by5joE45"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhP_s4YpoFqY"
      },
      "outputs": [],
      "source": [
        "# Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convert true labels to one-hot encoding\n",
        "num_classes = 3  # Negative, Neutral, Positive\n",
        "y_true_one_hot = np.eye(num_classes)[y_true]\n",
        "\n",
        "# Define colors for each class\n",
        "colors = [\"red\", \"blue\", \"green\"]\n",
        "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "\n",
        "# Create subplots: One plot per class\n",
        "fig, axes = plt.subplots(1, num_classes, figsize=(15, 5), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    precision, recall, _ = precision_recall_curve(y_true_one_hot[:, i], y_pred_proba[:, i])\n",
        "\n",
        "    ax.plot(recall, precision, color=colors[i], linewidth=2)  # Smoother lines\n",
        "    ax.set_title(f\"PR Curve - {class_names[i]}\")\n",
        "    ax.set_xlabel(\"Recall\")\n",
        "    ax.set_ylabel(\"Precision\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "# Adjust layout for clarity\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHZRF1HWoLnm"
      },
      "outputs": [],
      "source": [
        "# Accuracy Before vs After SMOTE\n",
        "accuracy_before_smote = 0.6524\n",
        "accuracy_after_smote = accuracy\n",
        "\n",
        "# Create a bar plot for comparison\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.bar(\n",
        "    [\"Before SMOTE\", \"After SMOTE\"],\n",
        "    [accuracy_before_smote, accuracy_after_smote],\n",
        "    color=[\"red\", \"green\"]\n",
        ")\n",
        "\n",
        "# Adding labels and title\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy: \\nBefore vs After Data Augmentation and SMOTE\\n\", fontsize=14)\n",
        "plt.ylim(0.0, 1.0)  # To show the full range of accuracy (0 to 1)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCNgnHMstWap"
      },
      "outputs": [],
      "source": [
        "# Sentiment Distribution of Financial News\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.histplot(all_predictions, bins=3, kde=False, color=\"purple\", discrete=True)\n",
        "#plt.xticks([0, 1, 2], labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
        "plt.xticks(ticks=[0, 1, 2], labels=[\"Negative\", \"Neutral\", \"Positive\"], fontsize=12)\n",
        "plt.title(\"Sentiment Distribution of Model Predictions\")\n",
        "plt.xlabel(\"Predicted Sentiment Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zd22OWzwTpL"
      },
      "outputs": [],
      "source": [
        "# ROC-AUC Curve for Each Sentiment Class\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Convert labels to one-hot encoding for multi-class ROC\n",
        "y_true_one_hot = label_binarize(all_labels, classes=[0, 1, 2])  # Assuming [Negative=0, Neutral=1, Positive=2]\n",
        "y_pred_proba = np.array(all_predictions_proba)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "class_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "colors = [\"red\", \"blue\", \"green\"]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], y_pred_proba[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    ax.plot(fpr, tpr, color=colors[i], lw=2, label=f\"AUC = {roc_auc:.2f}\")\n",
        "    ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "    ax.set_title(f\"ROC Curve - {class_labels[i]}\")\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN9nlhpXaOba"
      },
      "source": [
        "***DEPLOYMENT***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "semDbPNJaJem"
      },
      "outputs": [],
      "source": [
        "!pip install gradio torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr4VoTuhasFt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define model path\n",
        "model_path = \"finbert_sentiment_model\"\n",
        "\n",
        "# Save trained model\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "# Save optimizer state (optional, for resuming training)\n",
        "torch.save(optimizer.state_dict(), model_path + \"/optimizer.pt\")\n",
        "\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWqbkbsqazjJ"
      },
      "outputs": [],
      "source": [
        "# Load the Trained Model & Tokenizer for Deployment\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load trained model & tokenizer\n",
        "model_path = \"finbert_sentiment_model\"  # Path where model is saved\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Define sentiment labels\n",
        "sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrLXomIKa9QX"
      },
      "outputs": [],
      "source": [
        "# Define Sentiment Prediction Function\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Predicts sentiment for given financial news text with rule-based adjustments.\"\"\"\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "\n",
        "    # Get highest probability label\n",
        "    predicted_class = torch.argmax(probs, dim=1).item()\n",
        "    confidence = probs[0, predicted_class].item()\n",
        "\n",
        "    # Sentiment labels\n",
        "    sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "    predicted_label = sentiment_labels[predicted_class]\n",
        "\n",
        "    # RULE-BASED CORRECTIONS\n",
        "    negative_keywords = [\"crash\", \"plunge\", \"sell-off\", \"recession\", \"inflation\", \"cut jobs\", \"layoffs\", \"bankruptcy\", \"decline\"]\n",
        "    positive_keywords = [\"rally\", \"soar\", \"record high\", \"growth\", \"profit\", \"recovery\"]\n",
        "    neutral_bias_corrections = [\"Federal Reserve\", \"interest rate\", \"stimulus\"]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # If Neutral but contains clear negative words - Change to Negative\n",
        "    if predicted_label == \"Neutral\" and any(word in text_lower for word in negative_keywords):\n",
        "        predicted_label = \"Negative\"\n",
        "\n",
        "    # If Neutral but contains clear positive words - Change to Positive\n",
        "    elif predicted_label == \"Neutral\" and any(word in text_lower for word in positive_keywords):\n",
        "        predicted_label = \"Positive\"\n",
        "\n",
        "    # If Positive but contains negative indicators (like Fed rate hikes) - Reduce confidence\n",
        "    if predicted_label == \"Positive\" and any(word in text_lower for word in neutral_bias_corrections):\n",
        "        confidence *= 0.8\n",
        "\n",
        "    return f\"Sentiment: {predicted_label} (Confidence: {confidence:.2f})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reiSMN-QbEdT"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Deploy using Gradio\n",
        "interface = gr.Interface(\n",
        "    fn=predict_sentiment,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter financial news text...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Financial Sentiment Analysis\",\n",
        "    description=\"Enter financial news headlines or reports to analyze their sentiment (Positive, Neutral, Negative).\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "interface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}